{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c5944e",
   "metadata": {},
   "source": [
    "# PART 1: Code of Model for Translation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#Libraries imports#\n",
    "###################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, TimeDistributed, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#Data processing#\n",
    "#################\n",
    "\n",
    "#Load and split the dataset\n",
    "full_dataset = pd.read_csv('eng_french.csv')\n",
    "train_data, test_data = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.1, random_state=42)  # 10% for validation\n",
    "\n",
    "\n",
    "#Function to calculate the max length for padding\n",
    "def calculate_max_length(sentences, percentile=95):\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    return int(np.percentile(sentence_lengths, percentile))\n",
    "\n",
    "#Calculate max_length based on the dataset\n",
    "eng_lengths = calculate_max_length(full_dataset['English words/sentences'])\n",
    "fr_lengths = calculate_max_length(full_dataset['French words/sentences'])\n",
    "max_length = max(eng_lengths, fr_lengths)\n",
    "print(f\"Chosen max_length: {max_length}\")\n",
    "\n",
    "#Tokenizer fitting\n",
    "eng_tokenizer = Tokenizer()\n",
    "fr_tokenizer = Tokenizer()\n",
    "    #English Tockenizer\n",
    "eng_tokenizer.fit_on_texts(full_dataset['English words/sentences'])\n",
    "    #French Tockenizer\n",
    "fr_tokenizer.fit_on_texts(full_dataset['French words/sentences'])\n",
    "\n",
    "#Tokenization and padding functions\n",
    "def text_to_sequences(tokenizer, text):\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "\n",
    "#Padding\n",
    "def sequence_padding(sequences, length=None):\n",
    "    return pad_sequences(sequences, maxlen=length, padding='post')\n",
    "\n",
    "#Data Generator function\n",
    "#This part is used her but not necessary, it is to train the model on a very large dataset of millions data\n",
    "\n",
    "def data_generator(data, batch_size, tokenizer_en, tokenizer_fr, max_length):\n",
    "    while True:\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            chunk = data[i:i+batch_size]\n",
    "            english_sentences = chunk['English words/sentences']\n",
    "            french_sentences = chunk['French words/sentences']\n",
    "\n",
    "            eng_sequences = text_to_sequences(tokenizer_en, english_sentences)\n",
    "            fr_sequences = text_to_sequences(tokenizer_fr, french_sentences)\n",
    "\n",
    "            eng_sequences = sequence_padding(eng_sequences, max_length)\n",
    "            fr_sequences = sequence_padding(fr_sequences, max_length)\n",
    "\n",
    "            yield fr_sequences, eng_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a677c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#Creation and training of the model#\n",
    "####################################\n",
    "\n",
    "#Model architecture\n",
    "def create_translation_model(input_dim, output_dim, eng_vocab_size, fr_vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=fr_vocab_size, output_dim=256, input_length=input_dim))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(TimeDistributed(Dense(eng_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Initialize and summarize the model\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "translation_model = create_translation_model(max_length, max_length, eng_vocab_size, fr_vocab_size)\n",
    "translation_model.summary()\n",
    "\n",
    "#Training the model with generator including validation data\n",
    "batch_size = 32\n",
    "history = translation_model.fit_generator(\n",
    "    data_generator(train_data, batch_size, eng_tokenizer, fr_tokenizer, max_length),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    validation_data=data_generator(validation_data, batch_size, eng_tokenizer, fr_tokenizer, max_length),\n",
    "    validation_steps=len(validation_data) // batch_size,\n",
    "    epochs=10)\n",
    "\n",
    "#Plotting training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed28464",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Model Testing#\n",
    "###############\n",
    "\n",
    "#Preprocess the test data\n",
    "test_english = test_data['English words/sentences']\n",
    "test_french = test_data['French words/sentences']\n",
    "\n",
    "test_eng_sequences = text_to_sequences(eng_tokenizer, test_english)\n",
    "test_fr_sequences = text_to_sequences(fr_tokenizer, test_french)\n",
    "\n",
    "test_eng_sequences = sequence_padding(test_eng_sequences, max_length)\n",
    "test_fr_sequences = sequence_padding(test_fr_sequences, max_length)\n",
    "\n",
    "#Evaluate the model on test data\n",
    "test_loss, test_accuracy = translation_model.evaluate(test_fr_sequences, test_eng_sequences, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419845e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#Saving the model and tockenizers for prediction#\n",
    "#################################################\n",
    "\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "#Save model\n",
    "translation_model.save('translation_model4.h5')\n",
    "\n",
    "#Save tokenizers\n",
    "with open('eng_tokenizer4.pkl', 'wb') as handle:\n",
    "    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('fr_tokenizer4.pkl', 'wb') as handle:\n",
    "    pickle.dump(fr_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c437f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6ddd2",
   "metadata": {},
   "source": [
    "# PART 2: This part is for prediction and can be used in another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e95071",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#Librairies imports#\n",
    "####################\n",
    "\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, TimeDistributed, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import pickle\n",
    "\n",
    "#################################\n",
    "#Tokenizer and padding functions#\n",
    "#################################\n",
    "\n",
    "def text_to_sequences(tokenizer, text):\n",
    "    return tokenizer.texts_to_sequences(text)\n",
    "def sequence_padding(sequences, length=None):\n",
    "    return pad_sequences(sequences, maxlen=length, padding='post')\n",
    "max_length = 12\n",
    "\n",
    "#################################\n",
    "#Translation prediction function#\n",
    "#################################\n",
    "\n",
    "def translate(input_text):\n",
    "    #Load the saved model and tokenizers\n",
    "    model = load_model('translation_model4.h5')\n",
    "    with open('eng_tokenizer4.pkl', 'rb') as handle:\n",
    "        eng_tokenizer = pickle.load(handle)\n",
    "    with open('fr_tokenizer4.pkl', 'rb') as handle:\n",
    "        fr_tokenizer = pickle.load(handle)\n",
    "\n",
    "    #Preprocess the input text\n",
    "    sequences = text_to_sequences(fr_tokenizer, [input_text])\n",
    "    padded_sequences = sequence_padding(sequences, max_length)\n",
    "\n",
    "    #Make a prediction\n",
    "    prediction = model.predict(padded_sequences)\n",
    "    predicted_sequence = np.argmax(prediction, axis=-1)[0]\n",
    "\n",
    "    #Reverse word indices for both languages\n",
    "    reverse_fr_word_index = dict(map(reversed, fr_tokenizer.word_index.items()))\n",
    "    reverse_eng_word_index = dict(map(reversed, eng_tokenizer.word_index.items()))\n",
    "\n",
    "    #Convert the predicted sequence to text with fallback to French word\n",
    "    translated_text = []\n",
    "    for idx, word_idx in enumerate(padded_sequences[0]):\n",
    "        if word_idx > 0:  # Ignore padding\n",
    "            french_word = reverse_fr_word_index.get(word_idx, '')\n",
    "            translated_word = reverse_eng_word_index.get(predicted_sequence[idx], french_word)\n",
    "            translated_text.append(translated_word)\n",
    "\n",
    "    return ' '.join(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d234ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#example#\n",
    "#########\n",
    "\n",
    "input_text = \"le ciel est bleu  \"   #Example French sentence\n",
    "translated_text = translate(input_text)\n",
    "print(translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
